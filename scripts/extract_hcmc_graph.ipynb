{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d227b03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√¨m th·∫•y file ../data/hcmc_graph_data.json, ƒëang t·∫£i checkpoint...\n",
      "ƒê√£ x·ª≠ l√Ω 0 chunks. Script s·∫Ω ti·∫øp t·ª•c t·ª´ n∆°i ƒë√£ d·ª´ng.\n",
      "ƒêang t·∫£i file ../data/pdf_chunks_cleaned_for_embed_v3.json...\n",
      "T·ªïng c·ªông c√≥ 5409 chunks. B·∫Øt ƒë·∫ßu l·ªçc v√† tr√≠ch xu·∫•t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x·ª≠ l√Ω chunks:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 2933/5409 [01:24<09:47,  4.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ƒêang l∆∞u checkpoint (20 t·ªïng c·ªông) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x·ª≠ l√Ω chunks:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 2988/5409 [02:26<40:23,  1.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ƒêang l∆∞u checkpoint (40 t·ªïng c·ªông) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x·ª≠ l√Ω chunks:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3554/5409 [03:42<01:37, 19.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ƒêang l∆∞u checkpoint (60 t·ªïng c·ªông) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang x·ª≠ l√Ω chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5409/5409 [04:55<00:00, 18.30it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ho√†n t·∫•t. ƒê√£ tr√≠ch xu·∫•t th√™m 70 chunks m·ªõi.\n",
      "ƒêang l∆∞u file cu·ªëi c√πng v√†o ../data/hcmc_graph_data.json...\n",
      "Xong! üöÄ\n",
      "File k·∫øt qu·∫£ c·ªßa b·∫°n l√†: ../data/hcmc_graph_data.json\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm # Th∆∞ vi·ªán ƒë·ªÉ xem thanh ti·∫øn tr√¨nh (progress bar)\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# C√ÄI ƒê·∫∂T API KEY M·ªöI C·ª¶A B·∫†N T·∫†I ƒê√ÇY\n",
    "# -----------------------------------------------------------------\n",
    "API_KEY = \"AIzaSyC-tMrLg9IzJrEZV9PoF8kSuTybFudoMD8\"\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 1. ƒê·ªãnh nghƒ©a Schema Prompt cho LLM\n",
    "#\n",
    "# >>>>> PH·∫¶N S·ª¨A L·ªñI V2 (D√ôNG .replace()) <<<<<\n",
    "#\n",
    "# 1. Ch√∫ng ta d√πng placeholder __TEXT_CHUNK_HERE__ (thay v√¨ {text_chunk})\n",
    "# 2. Ch√∫ng ta quay l·∫°i d√πng d·∫•u ngo·∫∑c nh·ªçn { } b√¨nh th∆∞·ªùng cho v√≠ d·ª• JSON\n",
    "#    v√¨ .replace() s·∫Ω kh√¥ng \"ƒë·ª•ng\" ƒë·∫øn ch√∫ng.\n",
    "#\n",
    "SCHEMA_PROMPT_TEMPLATE = \"\"\"\n",
    "B·∫°n l√† m·ªôt chuy√™n gia x√¢y d·ª±ng ƒë·ªì th·ªã tri th·ª©c (Knowledge Graph Expert).\n",
    "Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªçc ƒëo·∫°n vƒÉn b·∫£n sau v√† tr√≠ch xu·∫•t T·∫§T C·∫¢ c√°c th·ª±c th·ªÉ v√† m·ªëi quan h·ªá C√ì LI√äN QUAN ƒê·∫æN TH√ÄNH PH·ªê H·ªí CH√ç MINH (ho·∫∑c S√†i G√≤n, Gia ƒê·ªãnh, Ch·ª£ L·ªõn).\n",
    "\n",
    "N·∫øu vƒÉn b·∫£n kh√¥ng ch·ª©a th√¥ng tin v·ªÅ TP.HCM, h√£y tr·∫£ v·ªÅ m·ªôt JSON r·ªóng: {\"entities\": [], \"relationships\": []}.\n",
    "\n",
    "Schema (C√°c lo·∫°i n√∫t v√† quan h·ªá):\n",
    "- Nodes (N√∫t):\n",
    "  - Location: M·ªôt ƒë·ªãa ƒëi·ªÉm, ƒë·ªãa danh c·ª• th·ªÉ (v√≠ d·ª•: \"Dinh ƒê·ªôc L·∫≠p\", \"Ch·ª£ B·∫øn Th√†nh\", \"Qu·∫≠n 1\", \"TP. H·ªì Ch√≠ Minh\").\n",
    "  - Event: M·ªôt s·ª± ki·ªán vƒÉn h√≥a, l·ªãch s·ª≠ (v√≠ d·ª•: \"L·ªÖ h·ªôi √Åo d√†i\", \"Chi·∫øn d·ªãch H·ªì Ch√≠ Minh\").\n",
    "  - Figure: M·ªôt nh√¢n v·∫≠t l·ªãch s·ª≠, vƒÉn h√≥a (v√≠ d·ª•: \"T·∫£ qu√¢n L√™ VƒÉn Duy·ªát\").\n",
    "  - Transport: M·ªôt c√¥ng tr√¨nh giao th√¥ng (v√≠ d·ª•: \"S√¢n bay T√¢n S∆°n Nh·∫•t\", \"B·∫øn xe Mi·ªÅn ƒê√¥ng\").\n",
    "  - Organization: M·ªôt t·ªï ch·ª©c, c√¥ng ty (v√≠ d·ª•: \"B·∫£o t√†ng Ch·ª©ng t√≠ch Chi·∫øn tranh\", \"Nh√† h√°t Th√†nh ph·ªë\").\n",
    "\n",
    "- Relationships (Quan h·ªá):\n",
    "  - LOCATED_IN: (Ngu·ªìn: Location, ƒê√≠ch: Location) - V√≠ d·ª•: \"Dinh ƒê·ªôc L·∫≠p\" LOCATED_IN \"Qu·∫≠n 1\".\n",
    "  - TAKES_PLACE_AT: (Ngu·ªìn: Event, ƒê√≠ch: Location) - V√≠ d·ª•: \"L·ªÖ h·ªôi √Åo d√†i\" TAKES_PLACE_AT \"Ph·ªë ƒëi b·ªô Nguy·ªÖn Hu·ªá\".\n",
    "  - RELATED_TO: (Ngu·ªìn: B·∫•t k·ª≥, ƒê√≠ch: B·∫•t k·ª≥) - M·ªëi quan h·ªá chung chung n·∫øu kh√¥ng r√µ.\n",
    "  - FOUNDED_BY: (Ngu·ªìn: Organization/Location, ƒê√≠ch: Figure) - V√≠ d·ª•: \"LƒÉng √îng B√† Chi·ªÉu\" RELATED_TO \"T·∫£ qu√¢n L√™ VƒÉn Duy·ªát\".\n",
    "\n",
    "VƒÇN B·∫¢N ƒê·ªÇ TR√çCH XU·∫§T:\n",
    "---\n",
    "__TEXT_CHUNK_HERE__\n",
    "---\n",
    "\n",
    "H∆Ø·ªöNG D·∫™N OUTPUT:\n",
    "Ch·ªâ tr·∫£ l·ªùi b·∫±ng m·ªôt kh·ªëi JSON h·ª£p l·ªá. KH√îNG th√™m b·∫•t k·ª≥ vƒÉn b·∫£n n√†o tr∆∞·ªõc ho·∫∑c sau kh·ªëi JSON.\n",
    "ƒê·ªãnh d·∫°ng JSON output ph·∫£i nh∆∞ sau:\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\"label\": \"Location\", \"name\": \"Dinh ƒê·ªôc L·∫≠p\"},\n",
    "    {\"label\": \"Location\", \"name\": \"Qu·∫≠n 1\"},\n",
    "    {\"label\": \"Figure\", \"name\": \"T·∫£ qu√¢n L√™ VƒÉn Duy·ªát\"}\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    {\"source\": \"Dinh ƒê·ªôc L·∫≠p\", \"target\": \"Qu·∫≠n 1\", \"type\": \"LOCATED_IN\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# 2. H√†m g·ªçi API Gemini\n",
    "def call_gemini_extraction(text_chunk, model):\n",
    "    \"\"\"\n",
    "    G·ªçi API Gemini ƒë·ªÉ tr√≠ch xu·∫•t graph data t·ª´ text.\n",
    "    C√≥ retry logic v√† x·ª≠ l√Ω l·ªói.\n",
    "    \"\"\"\n",
    "    \n",
    "    # >>>>> S·ª¨A L·ªñI: D√πng .replace() thay v√¨ .format() <<<<<\n",
    "    # Vi·ªác n√†y 100% an to√†n v√† kh√¥ng th·ªÉ g√¢y ra KeyError\n",
    "    prompt = SCHEMA_PROMPT_TEMPLATE.replace(\"__TEXT_CHUNK_HERE__\", text_chunk)\n",
    "    \n",
    "    retries = 3\n",
    "    delay = 5 # Ch·ªù 5 gi√¢y n·∫øu l·ªói\n",
    "\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                # T·∫Øt c√°c b·ªô l·ªçc an to√†n ƒë·ªÉ tr√°nh block nh·∫ßm l·∫´n text l·ªãch s·ª≠/vƒÉn h√≥a\n",
    "                safety_settings={\n",
    "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p LLM tr·∫£ v·ªÅ markdown ```json ... ```\n",
    "            raw_text = response.text.strip()\n",
    "            if raw_text.startswith(\"```json\"):\n",
    "                raw_text = raw_text[7:] # B·ªè \"```json\"\n",
    "            elif raw_text.startswith(\"```\"):\n",
    "                raw_text = raw_text[3:] # B·ªè \"```\"\n",
    "\n",
    "            if raw_text.endswith(\"```\"):\n",
    "                raw_text = raw_text[:-3] # B·ªè \"```\" ·ªü cu·ªëi\n",
    "            \n",
    "            # Parse JSON\n",
    "            # Th√™m ki·ªÉm tra n·∫øu raw_text r·ªóng\n",
    "            if not raw_text:\n",
    "                return {\"entities\": [], \"relationships\": []}\n",
    "                \n",
    "            return json.loads(raw_text)\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            # print(f\"!!! L·ªói JSONDecodeError: Kh√¥ng th·ªÉ parse output. Output th√¥: {raw_text}\")\n",
    "            # Tr·∫£ v·ªÅ r·ªóng ƒë·ªÉ script kh√¥ng b·ªã d·ª´ng\n",
    "            return {\"entities\": [], \"relationships\": []}\n",
    "        except Exception as e:\n",
    "            print(f\"!!! L·ªói API (l·ªói: {e}). ƒêang th·ª≠ l·∫°i sau {delay}s... (c√≤n {retries-1} l·∫ßn)\")\n",
    "            retries -= 1\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    print(f\"!!! B·ªè qua chunk n√†y sau 3 l·∫ßn th·ª≠ th·∫•t b·∫°i.\")\n",
    "    return None # Tr·∫£ v·ªÅ None n·∫øu th·∫•t b·∫°i 3 l·∫ßn\n",
    "\n",
    "# 3. H√†m main x·ª≠ l√Ω logic\n",
    "def main():\n",
    "    if API_KEY == \"D√ÅN_KEY_M·ªöI_C·ª¶A_B·∫†N_V√ÄO_ƒê√ÇY\" or API_KEY == \"YOUR_GEMINI_API_KEY_HERE\":\n",
    "        print(\"=\"*50)\n",
    "        print(\"L·ªñI: B·∫°n ch∆∞a d√°n API Key M·ªöI v√†o bi·∫øn 'API_KEY'.\")\n",
    "        print(\"H√£y l·∫•y API Key t·ª´: https://makersuite.google.com/\")\n",
    "        print(\"=\"*50)\n",
    "        return\n",
    "\n",
    "    # C·∫•u h√¨nh API\n",
    "    genai.configure(api_key=API_KEY)\n",
    "    generation_config = {\"temperature\": 0.0} # C·∫ßn k·∫øt qu·∫£ nh·∫•t qu√°n\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash-lite\", # thay ƒë·ªïi model ƒë∆∞·ª£c nhe\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "    # C√°c file v√† t·ª´ kh√≥a\n",
    "    # Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n n√†y n·∫øu c·∫ßn\n",
    "    INPUT_FILE = \"../data/pdf_chunks_cleaned_for_embed_v3.json\"\n",
    "    OUTPUT_FILE = \"../data/hcmc_graph_data.json\"\n",
    "    \n",
    "    # T·ª´ kh√≥a ƒë·ªÉ l·ªçc (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng) - PHI√äN B·∫¢N M·ªû R·ªòNG T·ªêI ƒêA\n",
    "    hcmc_keywords = [\n",
    "        # ----------------------------------------------------\n",
    "        # T√™n g·ªçi ch√≠nh th·ª©c, l·ªãch s·ª≠, v√† t√™n th∆∞·ªùng g·ªçi\n",
    "        # ----------------------------------------------------\n",
    "        'h·ªì ch√≠ minh', 's√†i g√≤n', 'tp.hcm', 'saigon',\n",
    "        's√†i g√≤n - ch·ª£ l·ªõn', 's√†i g√≤n - gia ƒë·ªãnh', 'h√≤n ng·ªçc vi·ªÖn ƒë√¥ng',\n",
    "        'gia ƒë·ªãnh th√†nh', 'phi√™n an', 'th√†nh ph·ªë h·ªì ch√≠ minh',\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # ƒê∆°n v·ªã h√†nh ch√≠nh: Qu·∫≠n (Urban Districts)\n",
    "        # ----------------------------------------------------\n",
    "        'qu·∫≠n 1', 'qu·∫≠n nh·∫•t', 'qu·∫≠n 3', 'qu·∫≠n 4', 'qu·∫≠n 5', 'qu·∫≠n 6',\n",
    "        'qu·∫≠n 7', 'qu·∫≠n 8', 'qu·∫≠n 10', 'qu·∫≠n 11', 'qu·∫≠n 12', 'ph√∫ nhu·∫≠n',\n",
    "        'b√¨nh th·∫°nh', 'g√≤ v·∫•p', 't√¢n b√¨nh', 't√¢n ph√∫', 'b√¨nh t√¢n',\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # ƒê∆°n v·ªã h√†nh ch√≠nh: TP & Huy·ªán (Suburban/Cities)\n",
    "        # ----------------------------------------------------\n",
    "        'th√†nh ph·ªë th·ªß ƒë·ª©c', 'tp th·ªß ƒë·ª©c', 'th·ªß ƒë·ª©c',\n",
    "        'huy·ªán c·ªß chi', 'c·ªß chi', 'huy·ªán h√≥c m√¥n', 'h√≥c m√¥n',\n",
    "        'huy·ªán b√¨nh ch√°nh', 'b√¨nh ch√°nh', 'huy·ªán nh√† b√®', 'nh√† b√®',\n",
    "        'huy·ªán c·∫ßn gi·ªù', 'c·∫ßn gi·ªù',\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Khu v·ª±c n·ªïi ti·∫øng / T√™n l·ªãch s·ª≠\n",
    "        # ----------------------------------------------------\n",
    "        'b·∫øn ngh√©', 'ch·ª£ l·ªõn', 'cholon', 'ƒë·ªÅ ng·∫°n', 'ph√∫ m·ªπ h∆∞ng',\n",
    "        'khu ƒë√¥ th·ªã m·ªõi th·ªß thi√™m', 'th·ªß thi√™m', 'ph·ªë t√¢y b√πi vi·ªán', 'b√πi vi·ªán',\n",
    "        'ph·∫°m ng≈© l√£o', 't√¢n c·∫£ng', 'b√¨nh qu·ªõi', 'l√†ng du l·ªãch b√¨nh qu·ªõi', 'ƒë·∫•t th√©p',\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Di t√≠ch / Bi·ªÉu t∆∞·ª£ng / T√≤a nh√†\n",
    "        # ----------------------------------------------------\n",
    "        'dinh ƒë·ªôc l·∫≠p', 'dinh th·ªëng nh·∫•t', 'h·ªôi tr∆∞·ªùng th·ªëng nh·∫•t',\n",
    "        'nh√† th·ªù ƒë·ª©c b√†', 'nh√† th·ªù ch√≠nh t√≤a ƒë·ª©c b√† s√†i g√≤n',\n",
    "        'b∆∞u ƒëi·ªán trung t√¢m', 'b∆∞u ƒëi·ªán th√†nh ph·ªë',\n",
    "        'nh√† h√°t th√†nh ph·ªë', 'nh√† h√°t l·ªõn s√†i g√≤n',\n",
    "        '·ªßy ban nh√¢n d√¢n th√†nh ph·ªë', 't√≤a ƒë√¥ ch√≠nh s√†i g√≤n', 'h√¥tel de ville',\n",
    "        'b·∫øn nh√† r·ªìng', 'b·∫£o t√†ng h·ªì ch√≠ minh', 'ch·ª£ b·∫øn th√†nh', 'c·∫ßu m√≥ng',\n",
    "        'h·ªì con r√πa', 'c√¥ng tr∆∞·ªùng qu·ªëc t·∫ø', 'th√°p t√†i ch√≠nh bitexco',\n",
    "        'bitexco financial tower', 'landmark 81', 't√≤a nh√† bitexco',\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # B·∫£o t√†ng\n",
    "        # ----------------------------------------------------\n",
    "        'b·∫£o t√†ng ch·ª©ng t√≠ch chi·∫øn tranh',\n",
    "        'b·∫£o t√†ng l·ªãch s·ª≠ th√†nh ph·ªë h·ªì ch√≠ minh',\n",
    "        'b·∫£o t√†ng th√†nh ph·ªë h·ªì ch√≠ minh', 'b·∫£o t√†ng m·ªπ thu·∫≠t',\n",
    "        'b·∫£o t√†ng m·ªπ thu·∫≠t tp.hcm', 'b·∫£o t√†ng √°o d√†i',\n",
    "        'b·∫£o t√†ng ph·ª• n·ªØ nam b·ªô', 'b·∫£o t√†ng fit√¥', 'b·∫£o t√†ng y h·ªçc c·ªï truy·ªÅn vi·ªát nam',\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Ch√πa / ƒê·ªÅn / Nh√† th·ªù / T√¥n gi√°o\n",
    "        # ----------------------------------------------------\n",
    "        'ch√πa vƒ©nh nghi√™m', 'ch√πa gi√°c l√¢m', 'ch√πa ng·ªçc ho√†ng', 'ph∆∞·ªõc h·∫£i t·ª±',\n",
    "        'ch√πa b√† thi√™n h·∫≠u', 'lƒÉng √¥ng b√† chi·ªÉu', 'lƒÉng t·∫£ qu√¢n l√™ vƒÉn duy·ªát',\n",
    "        'lƒÉng cha c·∫£', 'ƒë·ªÅn tr·∫ßn h∆∞ng ƒë·∫°o', 'ƒë·ªÅn h√πng',\n",
    "        'nh√† th·ªù t√¢n ƒë·ªãnh', 'nh√† th·ªù huy·ªán s·ªπ', 'th√°nh ƒë∆∞·ªùng h·ªìi gi√°o jamia',\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Ch·ª£ / Mua s·∫Øm\n",
    "        # ----------------------------------------------------\n",
    "        'ch·ª£ an ƒë√¥ng', 'ch·ª£ b√¨nh t√¢y', 'ch·ª£ t√¢n ƒë·ªãnh', 'ch·ª£ b√† chi·ªÉu', 'ch·ª£ kim bi√™n',\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # ƒê∆∞·ªùng ph·ªë / Qu·∫£ng tr∆∞·ªùng\n",
    "        # ----------------------------------------------------\n",
    "        'ƒë∆∞·ªùng nguy·ªÖn hu·ªá', 'ph·ªë ƒëi b·ªô nguy·ªÖn hu·ªá', 'ƒë∆∞·ªùng ƒë·ªìng kh·ªüi',\n",
    "        'ƒë∆∞·ªùng t·ª± do', 'ƒë∆∞·ªùng catinat', 'ƒë∆∞·ªùng l√™ l·ª£i', 'ƒë∆∞·ªùng pasteur',\n",
    "        'ƒë∆∞·ªùng hai b√† tr∆∞ng', 'ƒë∆∞·ªùng nam k·ª≥ kh·ªüi nghƒ©a', 'ƒë∆∞·ªùng l√™ du·∫©n',\n",
    "        'ƒë∆∞·ªùng t√¥n ƒë·ª©c th·∫Øng', 'ƒë·∫°i l·ªô ƒë√¥ng t√¢y', 'v√µ vƒÉn ki·ªát',\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Giao th√¥ng / H·∫° t·∫ßng\n",
    "        # ----------------------------------------------------\n",
    "        's√¢n bay t√¢n s∆°n nh·∫•t', 't√¢n s∆°n nh·ª©t', 'c·∫£ng h√†ng kh√¥ng qu·ªëc t·∫ø t√¢n s∆°n nh·∫•t',\n",
    "        'b·∫øn xe mi·ªÅn ƒë√¥ng', 'b·∫øn xe mi·ªÅn t√¢y', 'c·∫£ng s√†i g√≤n', 'c·∫ßu s√†i g√≤n',\n",
    "        'h·∫ßm th·ªß thi√™m', 'h·∫ßm s√¥ng s√†i g√≤n', 'ga s√†i g√≤n',\n",
    "        'tuy·∫øn metro s·ªë 1', 'b·∫øn th√†nh - su·ªëi ti√™n',\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # C√¥ng vi√™n / Gi·∫£i tr√≠ / Sinh th√°i\n",
    "        # ----------------------------------------------------\n",
    "        'th·∫£o c·∫ßm vi√™n', 'v∆∞·ªùn b√°ch th·∫£o', 'c√¥ng vi√™n 23/9', 'c√¥ng vi√™n 30/4',\n",
    "        'c√¥ng vi√™n l√™ vƒÉn t√°m', 'c√¥ng vi√™n tao ƒë√†n', 'c√¥ng vi√™n vƒÉn h√≥a ƒë·∫ßm sen',\n",
    "        'ƒë·∫ßm sen', 'khu du l·ªãch su·ªëi ti√™n', 'su·ªëi ti√™n', 'khu du l·ªãch vƒÉn th√°nh',\n",
    "        'ƒë·ªãa ƒë·∫°o c·ªß chi', 'r·ª´ng s√°c', 'khu d·ª± tr·ªØ sinh quy·ªÉn c·∫ßn gi·ªù',\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Nh√¢n v·∫≠t l·ªãch s·ª≠ (g·∫Øn li·ªÅn v·ªõi ƒë·ªãa danh)\n",
    "        # ----------------------------------------------------\n",
    "        'l√™ vƒÉn duy·ªát', 'tr∆∞∆°ng ƒë·ªãnh', 'nguy·ªÖn h·ªØu c·∫£nh', 'phan thanh gi·∫£n',\n",
    "    ]\n",
    "\n",
    "    # Ki·ªÉm tra checkpoint\n",
    "    processed_chunk_ids = set()\n",
    "    output_data = []\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        print(f\"T√¨m th·∫•y file {OUTPUT_FILE}, ƒëang t·∫£i checkpoint...\")\n",
    "        try:\n",
    "            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "                output_data = json.load(f)\n",
    "                for item in output_data:\n",
    "                    processed_chunk_ids.add(item['chunk_id'])\n",
    "            print(f\"ƒê√£ x·ª≠ l√Ω {len(processed_chunk_ids)} chunks. Script s·∫Ω ti·∫øp t·ª•c t·ª´ n∆°i ƒë√£ d·ª´ng.\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"File {OUTPUT_FILE} b·ªã l·ªói. S·∫Ω ghi ƒë√® file m·ªõi.\")\n",
    "            output_data = []\n",
    "    \n",
    "    # ƒê·ªçc file chunk\n",
    "    print(f\"ƒêang t·∫£i file {INPUT_FILE}...\")\n",
    "    try:\n",
    "        with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            all_chunks = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"L·ªñI: Kh√¥ng t√¨m th·∫•y file {INPUT_FILE}.\")\n",
    "        print(f\"H√£y ch·∫Øc ch·∫Øn file c·ªßa b·∫°n n·∫±m ·ªü: {os.path.abspath(INPUT_FILE)}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªñI: Kh√¥ng th·ªÉ ƒë·ªçc file {INPUT_FILE}. L·ªói: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"T·ªïng c·ªông c√≥ {len(all_chunks)} chunks. B·∫Øt ƒë·∫ßu l·ªçc v√† tr√≠ch xu·∫•t...\")\n",
    "    \n",
    "    count_new = 0\n",
    "    try:\n",
    "        # S·ª≠ d·ª•ng tqdm ƒë·ªÉ xem thanh ti·∫øn tr√¨nh\n",
    "        for chunk in tqdm(all_chunks, desc=\"ƒêang x·ª≠ l√Ω chunks\"):\n",
    "            \n",
    "            # Ph·∫£i ki·ªÉm tra 'id' v√† 'text' t·ªìn t·∫°i trong chunk kh√¥ng\n",
    "            if 'id' not in chunk or 'text' not in chunk:\n",
    "                continue # B·ªè qua chunk n√†y n·∫øu kh√¥ng c√≥ key 'id' ho·∫∑c 'text'\n",
    "            \n",
    "            chunk_id = chunk['id']\n",
    "            text = chunk['text'] \n",
    "            text_lower = text.lower()\n",
    "\n",
    "            # 1. B·ªè qua n·∫øu ƒë√£ x·ª≠ l√Ω (t·ª´ checkpoint)\n",
    "            if chunk_id in processed_chunk_ids:\n",
    "                continue\n",
    "            \n",
    "            # 2. L·ªçc t·ª´ kh√≥a\n",
    "            if not any(keyword in text_lower for keyword in hcmc_keywords):\n",
    "                continue\n",
    "            \n",
    "            # 3. G·ªçi LLM ƒë·ªÉ tr√≠ch xu·∫•t\n",
    "            # print(f\"\\nƒêang x·ª≠ l√Ω chunk {chunk_id} (Trang: {chunk.get('page', 'N/A')})...\")\n",
    "            graph_data = call_gemini_extraction(text, model)\n",
    "            \n",
    "            if graph_data is None: # L·ªói API kh√¥ng th·ªÉ retry\n",
    "                continue\n",
    "\n",
    "            # 4. L∆∞u k·∫øt qu·∫£ n·∫øu c√≥\n",
    "            if graph_data.get('entities') or graph_data.get('relationships'):\n",
    "                # print(f\"--> T√¨m th·∫•y {len(graph_data['entities'])} th·ª±c th·ªÉ, {len(graph_data['relationships'])} quan h·ªá.\")\n",
    "                output_data.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"page\": chunk.get('page', 'N/A'),\n",
    "                    \"chunk_text\": text,\n",
    "                    \"graph_data\": graph_data\n",
    "                })\n",
    "                count_new += 1\n",
    "            # else:\n",
    "                # print(f\"--> Kh√¥ng t√¨m th·∫•y th·ª±c th·ªÉ n√†o li√™n quan ƒë·∫øn TP.HCM.\")\n",
    "\n",
    "            # 5. L∆∞u checkpoint m·ªói 20 chunks m·ªõi\n",
    "            if count_new > 0 and count_new % 20 == 0:\n",
    "                tqdm.write(f\"--- ƒêang l∆∞u checkpoint ({len(output_data)} t·ªïng c·ªông) ---\")\n",
    "                with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            # 6. Rate limit - ch·ªù 1 gi√¢y gi·ªØa c√°c request\n",
    "            # API Gemini Flash mi·ªÖn ph√≠ c√≥ gi·ªõi h·∫°n 60 request/ph√∫t.\n",
    "            time.sleep(1.1) \n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nPh√°t hi·ªán ng·∫Øt (Ctrl+C). ƒêang l∆∞u... C·∫£m phi·ªÅn ch·ªù.\")\n",
    "    \n",
    "    finally:\n",
    "        # L∆∞u l·∫ßn cu·ªëi\n",
    "        print(f\"Ho√†n t·∫•t. ƒê√£ tr√≠ch xu·∫•t th√™m {count_new} chunks m·ªõi.\")\n",
    "        print(f\"ƒêang l∆∞u file cu·ªëi c√πng v√†o {OUTPUT_FILE}...\")\n",
    "        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "        print(\"Xong! üöÄ\")\n",
    "        print(f\"File k·∫øt qu·∫£ c·ªßa b·∫°n l√†: {OUTPUT_FILE}\")\n",
    "        print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
