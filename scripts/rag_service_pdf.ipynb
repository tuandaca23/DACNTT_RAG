{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82eea76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(\"../data\")\n",
    "INDEX_PATH = OUT_DIR / \"pdf_index.faiss\"\n",
    "EMBEDDINGS_PATH = OUT_DIR / \"pdf_embeddings.npy\"\n",
    "CHUNKS_PATH = OUT_DIR / \"pdf_chunks.json\"\n",
    "\n",
    "def l2_normalize(a, axis=1, eps=1e-10):\n",
    "    norm = np.linalg.norm(a, axis=axis, keepdims=True)\n",
    "    return a / (norm + eps)\n",
    "\n",
    "class RAGService:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                 generator_model=None):  # generator_model=None => dùng extractive fallback\n",
    "        # embed model\n",
    "        self.embed_model = SentenceTransformer(model_name)\n",
    "\n",
    "        # load chunks (ensure order consistent with embeddings file)\n",
    "        with open(CHUNKS_PATH, 'r', encoding='utf-8') as f:\n",
    "            self.chunks = json.load(f)  # list of dicts with id,page,text\n",
    "\n",
    "        # load embeddings\n",
    "        self.embeddings = np.load(EMBEDDINGS_PATH)  # should be shape (N, D)\n",
    "\n",
    "        # sanity checks\n",
    "        if len(self.embeddings) != len(self.chunks):\n",
    "            raise ValueError(f\"Embeddings length ({len(self.embeddings)}) != chunks ({len(self.chunks)}) — check you saved them in the same order\")\n",
    "\n",
    "        # ensure dtype float32\n",
    "        if self.embeddings.dtype != np.float32:\n",
    "            print(\"[WARN] embeddings dtype != float32, converting.\")\n",
    "            self.embeddings = self.embeddings.astype(\"float32\")\n",
    "\n",
    "        # normalize embeddings for cosine similarity and build index (IndexFlatIP)\n",
    "        self.embeddings = l2_normalize(self.embeddings)\n",
    "        dim = self.embeddings.shape[1]\n",
    "\n",
    "        # Build a new FAISS index in-memory (safe) from loaded embeddings to avoid mismatch issues\n",
    "        self.index = faiss.IndexFlatIP(dim)  # inner product on normalized vectors == cosine\n",
    "        self.index.add(self.embeddings)\n",
    "        # (optional) write index back if you want\n",
    "        faiss.write_index(self.index, str(INDEX_PATH))\n",
    "\n",
    "        # generator: optional. If provided, it MUST be a causal/generation-capable model.\n",
    "        if generator_model:\n",
    "            # NOTE: pass device_map or device if needed. Use a known causal Vietnamese model.\n",
    "            self.generator = pipeline('text-generation', model=generator_model)\n",
    "        else:\n",
    "            self.generator = None\n",
    "\n",
    "        print(\"✅ RAGService ready. #chunks:\", len(self.chunks), \"dim:\", dim)\n",
    "\n",
    "    def retrieve(self, query, top_k=5):\n",
    "        q_emb = self.embed_model.encode([query], convert_to_numpy=True)\n",
    "        if q_emb.dtype != np.float32:\n",
    "            q_emb = q_emb.astype(\"float32\")\n",
    "        q_emb = l2_normalize(q_emb)\n",
    "        D, I = self.index.search(q_emb, top_k)  # returns inner product scores (higher = closer)\n",
    "        results = []\n",
    "        for score, idx in zip(D[0], I[0]):\n",
    "            if idx == -1: continue\n",
    "            # safety bounds\n",
    "            if idx < 0 or idx >= len(self.chunks): continue\n",
    "            results.append({\n",
    "                \"id\": self.chunks[idx].get(\"id\"),\n",
    "                \"page\": self.chunks[idx].get(\"page\"),\n",
    "                \"text\": self.chunks[idx].get(\"text\"),\n",
    "                \"score\": float(score)  # cosine-ish similarity in [-1,1]\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def generate_or_extract(self, query, top_k=5, use_generator=False):\n",
    "        retrieved = self.retrieve(query, top_k=top_k)\n",
    "        # debug: always print top retrieved\n",
    "        print(\"DEBUG: Retrieved top-k:\")\n",
    "        for r in retrieved:\n",
    "            print(f\" - id={r['id']} page={r['page']} score={r['score']:.4f} text_preview={r['text'][:120]!r}\")\n",
    "\n",
    "        # simple extractive fallback:\n",
    "        # if top score is high enough, return that chunk (or the sentence inside it)\n",
    "        if len(retrieved) == 0:\n",
    "            return {\"answer\": \"\", \"sources\": retrieved, \"method\": \"none\"}\n",
    "\n",
    "        top = retrieved[0]\n",
    "        if top['score'] >= 0.5 or not use_generator:\n",
    "            # try to extract a direct answer sentence that contains keywords from query\n",
    "            q_words = set([w.lower() for w in query.split() if len(w) > 1])\n",
    "            sentences = [s.strip() for s in top['text'].split('.') if s.strip()]\n",
    "            # prefer sentence that contains most query words\n",
    "            best_sent, best_count = None, -1\n",
    "            for s in sentences:\n",
    "                cnt = sum(1 for w in q_words if w in s.lower())\n",
    "                if cnt > best_count:\n",
    "                    best_count = cnt\n",
    "                    best_sent = s\n",
    "            answer = best_sent if best_sent else top['text'][:400]\n",
    "            return {\"answer\": answer.strip(), \"sources\": retrieved, \"method\": \"extractive\", \"score\": top['score']}\n",
    "\n",
    "        # else call generator (if available)\n",
    "        if self.generator:\n",
    "            context = \" \".join([r[\"text\"] for r in retrieved])\n",
    "            prompt = f\"Ngữ cảnh: {context}\\nCâu hỏi: {query}\\nTrả lời ngắn gọn, chính xác bằng tiếng Việt:\"\n",
    "            # deterministic generation\n",
    "            gen = self.generator(prompt, max_new_tokens=128, do_sample=False, num_return_sequences=1)[0]['generated_text']\n",
    "            # remove prompt echo if present\n",
    "            if gen.startswith(prompt):\n",
    "                gen = gen[len(prompt):].strip()\n",
    "            return {\"answer\": gen.strip(), \"sources\": retrieved, \"method\": \"generator\"}\n",
    "        else:\n",
    "            return {\"answer\": top['text'][:400].strip(), \"sources\": retrieved, \"method\": \"extractive (no generator)\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdebe197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAGService ready. #chunks: 3090 dim: 384\n",
      "DEBUG: Retrieved top-k:\n",
      " - id=2152 page=698 score=0.7710 text_preview='nam định'\n",
      " - id=454 page=153 score=0.7684 text_preview='gùi đan hoa văn bằng nan nhu ộm đen.'\n",
      " - id=806 page=259 score=0.7667 text_preview='đã có'\n",
      " - id=2164 page=702 score=0.7641 text_preview='cầu nam định'\n",
      " - id=1762 page=572 score=0.7599 text_preview='ngày nay đang được đầu tư khôi thành phố hồ chí minh'\n",
      "extractive\n",
      "nam định\n",
      "698 0.7709580063819885\n",
      "153 0.7683683633804321\n",
      "259 0.7667286396026611\n"
     ]
    }
   ],
   "source": [
    "svc = RAGService(generator_model=None)   # tạm không dùng generator\n",
    "res = svc.generate_or_extract(\"Nhà thờ Đức Bà ở đâu?\", top_k=5)\n",
    "print(res['method'])\n",
    "print(res['answer'])\n",
    "for s in res['sources'][:3]:\n",
    "    print(s['page'], s['score'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
