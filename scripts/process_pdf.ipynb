{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac288e9",
   "metadata": {},
   "source": [
    "### ƒê·ªçc v√† ti·ªÅn x·ª≠ l√Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80dd7e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ ƒë·ªçc 1027 trang.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking pages: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1027/1027 [00:00<00:00, 8318.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ T·ªïng chunk: 3892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç T·∫°o embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 122/122 [00:12<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ L∆∞u xong index + embeddings + chunks.\n"
     ]
    }
   ],
   "source": [
    "# preprocess_vanhoa_pdf.py (optimized)\n",
    "import re, json, unicodedata\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "PDF_PATH = Path(\"../data/VanHoaVaDuLichVN.pdf\")\n",
    "OUT_DIR = Path(\"../data\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ========== Cleaning improved ==========\n",
    "def clean_text_advanced(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    # Remove common headers/footers patterns and page numbers\n",
    "    s = re.sub(r'(?mi)^(?:page|trang)\\s*\\d+\\b', ' ', s)\n",
    "    s = re.sub(r'\\f', ' ', s)\n",
    "    # remove leading/trailing digits sticking to text like \"581Nh√†\"\n",
    "    s = re.sub(r'(?<=\\s)\\d{2,4}(?=[A-Za-z√Ä-·ªπ])', ' ', s)\n",
    "    s = re.sub(r'^\\s*\\d{1,4}\\s*', ' ', s)\n",
    "    s = re.sub(r'\\s*\\d{1,4}\\s*$', ' ', s)\n",
    "    # fix hyphenation/newline breaks and many whitespace\n",
    "    s = re.sub(r'(\\w)-\\s+(\\w)', r'\\1\\2', s)       # foo-\\nbar -> foobar\n",
    "    s = re.sub(r'[\\r\\n\\t]+', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    # Fix Vietnamese token fragmentation heuristics (merge 1-2 char tokens)\n",
    "    s = re.sub(r'\\b([a-zƒë√Ä-·ªπ]{1,2})\\s+([a-zƒë√Ä-·ªπ]{1,3})\\b', r'\\1\\2', s, flags=re.I)\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "# ========== Read PDF (PyPDF2; can swap to pdfminer) ==========\n",
    "def read_pdf(pdf_path: Path):\n",
    "    reader = PyPDF2.PdfReader(str(pdf_path))\n",
    "    pages = []\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        raw = page.extract_text() or \"\"\n",
    "        cleaned = clean_text_advanced(raw)\n",
    "        if cleaned:\n",
    "            pages.append({\"page\": i+1, \"text\": cleaned})\n",
    "    print(f\"‚úÖ ƒê√£ ƒë·ªçc {len(pages)} trang.\")\n",
    "    return pages\n",
    "\n",
    "# ========== Sentence split & chunk ==========\n",
    "def split_sentences(text: str):\n",
    "    sents = re.split(r'(?<=[\\.\\?\\!‚Ä¶])\\s+', text)\n",
    "    return [s.strip() for s in sents if s.strip()]\n",
    "\n",
    "def chunk_by_sentences(pages, max_words=300, overlap_sentences=2):\n",
    "    chunks = []\n",
    "    cid = 0\n",
    "    for p in tqdm(pages, desc=\"Chunking pages\"):\n",
    "        sents = split_sentences(p[\"text\"])\n",
    "        i = 0\n",
    "        while i < len(sents):\n",
    "            cur, cnt = [], 0\n",
    "            j = i\n",
    "            while j < len(sents) and cnt + len(sents[j].split()) <= max_words:\n",
    "                cur.append(sents[j])\n",
    "                cnt += len(sents[j].split())\n",
    "                j += 1\n",
    "            if not cur:\n",
    "                cur = [sents[i]]\n",
    "                j = i + 1\n",
    "            cid += 1\n",
    "            text_chunk = \" \".join(cur)\n",
    "            chunks.append({\n",
    "                \"id\": cid,\n",
    "                \"page\": p[\"page\"],\n",
    "                \"text\": text_chunk,\n",
    "                \"first_sentence\": cur[0] if cur else \"\",\n",
    "                \"char_len\": len(text_chunk)\n",
    "            })\n",
    "            i = max(i + 1, j - overlap_sentences)\n",
    "    print(f\"‚úÖ T·ªïng chunk: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "# ========== Embed + FAISS (use multilingual model) ==========\n",
    "def embed_and_index(chunks, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    print(\"üîç T·∫°o embedding...\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True).astype(\"float32\")\n",
    "    # normalize for cosine\n",
    "    embeddings = embeddings / (np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-10)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings)\n",
    "    # Save\n",
    "    faiss.write_index(index, str(OUT_DIR / \"pdf_index.faiss\"))\n",
    "    np.save(OUT_DIR / \"pdf_embeddings.npy\", embeddings)\n",
    "    with open(OUT_DIR / \"pdf_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "    print(\"‚úÖ L∆∞u xong index + embeddings + chunks.\")\n",
    "    return index\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pages = read_pdf(PDF_PATH)\n",
    "    chunks = chunk_by_sentences(pages, max_words=300, overlap_sentences=2)\n",
    "    embed_and_index(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f54c15",
   "metadata": {},
   "source": [
    "### Re-chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdaec46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3892/3892 [00:52<00:00, 74.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Saved cleaned chunks: ..\\data\\pdf_chunks_cleaned.json (count=7688)\n",
      "Mapping saved: ..\\data\\pdf_chunks_mapping.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "M·ª•c ƒë√≠ch:\n",
    "- Load ../data/pdf_chunks.json (chunk hi·ªán c√≥)\n",
    "- Clean: fix hyphenation, remove page-number residues, separate digits/letters,\n",
    "  tokenise VN b·∫±ng underthesea n·∫øu c√≥, post-process punctuation\n",
    "- Re-chunk n·∫øu chunk qu√° d√†i (max_words configurable)\n",
    "- L∆∞u pdf_chunks_cleaned.json v√† mapping old_chunk_id -> new_chunk_ids\n",
    "\"\"\"\n",
    "\n",
    "import re, json, unicodedata\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# config\n",
    "IN_PATH = Path(\"../data/pdf_chunks.json\")\n",
    "OUT_PATH = Path(\"../data/pdf_chunks_cleaned.json\")\n",
    "MAPPING_PATH = Path(\"../data/pdf_chunks_mapping.json\")\n",
    "MAX_WORDS = 200          # t·ªëi ƒëa t·ª´ 1 chunk sau clean\n",
    "OVERLAP_SENTENCES = 1    # overlap (s·ªë c√¢u)\n",
    "KEEP_ORIGINAL_PAGE = True\n",
    "\n",
    "# try underthesea\n",
    "try:\n",
    "    from underthesea import word_tokenize, sent_tokenize\n",
    "    HAS_UT = True\n",
    "except Exception:\n",
    "    HAS_UT = False\n",
    "    print(\"[WARN] underthesea kh√¥ng kh·∫£ d·ª•ng ‚Äî tokenization fallback d√πng regex.\")\n",
    "\n",
    "# -----------------------------\n",
    "# cleaning function for chunk text\n",
    "# -----------------------------\n",
    "def clean_chunk_text(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "\n",
    "    # remove stray form feeds\n",
    "    s = s.replace('\\f', ' ')\n",
    "\n",
    "    # remove leading/trailing page numbers or isolated numbers at line starts\n",
    "    s = re.sub(r'(?m)^\\s*\\d{1,4}\\s*', ' ', s)\n",
    "    s = re.sub(r'\\s*\\d{1,4}\\s*$', ' ', s)\n",
    "\n",
    "    # fix hyphenation broken across line breaks or spaces: \"th - ·ªù\" / \"foo-\\nbar\"\n",
    "    s = re.sub(r'(\\w)-\\s+(\\w)', r'\\1\\2', s)\n",
    "    s = s.replace('\\u2013', '-').replace('\\u2014', '-')\n",
    "\n",
    "    # isolate punctuation so tokenizer can handle consistently\n",
    "    s = re.sub(r'([,.;:!?()\"‚Äú‚Äù¬´¬ª\\[\\]])', r' \\1 ', s)\n",
    "\n",
    "    # separate digits stuck to letters (e.g., \"1877nƒÉm\" -> \"1877 nƒÉm\")\n",
    "    s = re.sub(r'([0-9])([^\\s0-9\\W])', r'\\1 \\2', s)\n",
    "    s = re.sub(r'([^\\s0-9\\W])([0-9])', r'\\1 \\2', s)\n",
    "\n",
    "    # collapse whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "    # Vietnamese tokenization: prefer underthesea.word_tokenize (format=\"text\")\n",
    "    if HAS_UT:\n",
    "        try:\n",
    "            s_tok = word_tokenize(s, format=\"text\")\n",
    "        except Exception:\n",
    "            s_tok = s\n",
    "    else:\n",
    "        s_tok = s\n",
    "\n",
    "    # Post-process punctuation spacing\n",
    "    s_tok = re.sub(r'\\s+([,.;:!?()\\[\\]\"])', r'\\1', s_tok)\n",
    "    s_tok = re.sub(r'\\s+', ' ', s_tok).strip()\n",
    "\n",
    "    # lowercase optional (keep lowercase for embedding)\n",
    "    s_tok = s_tok.lower()\n",
    "\n",
    "    return s_tok\n",
    "\n",
    "# -----------------------------\n",
    "# sentence splitter (fallback to regex if underthesea not available)\n",
    "# -----------------------------\n",
    "def split_sentences(text: str):\n",
    "    if HAS_UT:\n",
    "        try:\n",
    "            sents = sent_tokenize(text)\n",
    "            # underthesea.sent_tokenize returns list\n",
    "            if isinstance(sents, list) and len(sents) > 0:\n",
    "                return [s.strip() for s in sents if s.strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # fallback simple regex split on punctuation\n",
    "    sents = re.split(r'(?<=[\\.\\?\\!‚Ä¶])\\s+', text)\n",
    "    return [s.strip() for s in sents if s.strip()]\n",
    "\n",
    "# -----------------------------\n",
    "# re-chunk sentences into chunks (max_words, overlap_sentences)\n",
    "# -----------------------------\n",
    "def re_chunk_from_text(text: str, page: int, source_chunk_id: int, start_new_cid):\n",
    "    sents = split_sentences(text)\n",
    "    new_chunks = []\n",
    "    cid = start_new_cid\n",
    "    i = 0\n",
    "    while i < len(sents):\n",
    "        cur = []\n",
    "        cnt = 0\n",
    "        j = i\n",
    "        while j < len(sents) and cnt + len(sents[j].split()) <= MAX_WORDS:\n",
    "            cur.append(sents[j])\n",
    "            cnt += len(sents[j].split())\n",
    "            j += 1\n",
    "        if not cur:\n",
    "            cur = [sents[i]]\n",
    "            j = i + 1\n",
    "        cid += 1\n",
    "        chunk_text = \" \".join(cur)\n",
    "        new_chunks.append({\n",
    "            \"id\": cid,\n",
    "            \"page\": page,\n",
    "            \"text\": chunk_text,\n",
    "            \"first_sentence\": cur[0] if cur else \"\",\n",
    "            \"char_len\": len(chunk_text),\n",
    "            \"source_chunk_id\": source_chunk_id\n",
    "        })\n",
    "        i = max(i + 1, j - OVERLAP_SENTENCES)\n",
    "    return new_chunks, cid\n",
    "\n",
    "# -----------------------------\n",
    "# main process\n",
    "# -----------------------------\n",
    "def main():\n",
    "    assert IN_PATH.exists(), f\"Kh√¥ng t√¨m th·∫•y {IN_PATH}\"\n",
    "    raw_chunks = json.load(open(IN_PATH, 'r', encoding='utf-8'))\n",
    "    cleaned_chunks = []\n",
    "    mapping = {}  # old_id -> list of new_ids\n",
    "\n",
    "    next_cid = 0\n",
    "    for c in tqdm(raw_chunks, desc=\"Processing chunks\"):\n",
    "        old_id = c.get(\"id\")\n",
    "        page = c.get(\"page\", None)\n",
    "        text = c.get(\"text\", \"\")\n",
    "        # 1) clean text\n",
    "        cleaned = clean_chunk_text(text)\n",
    "        # 2) re-chunk cleaned text to ensure chunks not too long and sentences intact\n",
    "        new_chunks, next_cid = re_chunk_from_text(cleaned, page if KEEP_ORIGINAL_PAGE else None, old_id, next_cid)\n",
    "        # append and map\n",
    "        cleaned_chunks.extend(new_chunks)\n",
    "        mapping[str(old_id)] = [nc[\"id\"] for nc in new_chunks]\n",
    "\n",
    "    # save\n",
    "    json.dump(cleaned_chunks, open(OUT_PATH, \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n",
    "    json.dump(mapping, open(MAPPING_PATH, \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Done. Saved cleaned chunks: {OUT_PATH} (count={len(cleaned_chunks)})\")\n",
    "    print(f\"Mapping saved: {MAPPING_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74fc53",
   "metadata": {},
   "source": [
    "### X·ª≠ l√Ω pdf_chunks_cleaned.json s·∫°ch s·∫Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cedbc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== B√ÅO C√ÅO T√ìM T·∫ÆT ===\n",
      "Chunks ban ƒë·∫ßu: 7688\n",
      "Sau lo·∫°i <50 k√Ω t·ª±: 6328\n",
      "Sau dedupe (pre-merge): 5320\n",
      "Sau g·ªôp: 5165\n",
      "Chunks cu·ªëi c√πng (ƒë√£ dedupe): 5165\n",
      "Exact duplicate (g·ªëc): 851\n",
      "Normalized duplicate (g·ªëc): 851\n",
      "Ph·∫ßn trƒÉm chunk c√≥ d·∫•u g·∫°ch d∆∞·ªõi (_): 92.60%\n",
      "ƒê·ªô d√†i (min/median/mean/max): 50/315.0/491.2/3917\n",
      "Bucket ƒë·ªô d√†i: {'0-49': 0, '50-99': 675, '100-199': 1105, '200-399': 1394, '400-799': 894, '800-1999': 1010, '2000+': 87}\n",
      "S·ªë chunk qu√° d√†i (>2000): 87\n",
      "File ƒë·∫ßu ra: ../data/pdf_chunks_cleaned_for_embed.json\n"
     ]
    }
   ],
   "source": [
    "# X·ª≠ l√Ω chunks ƒë·ªÉ chu·∫©n b·ªã cho embedding.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "# ===== C·∫§U H√åNH =====\n",
    "IN_PATH  = \"../data/pdf_chunks_cleaned.json\"\n",
    "OUT_PATH = \"../data/pdf_chunks_cleaned_for_embed.json\"\n",
    "\n",
    "SHORT_DROP_CHARLEN = 50      # lo·∫°i chunk < N k√Ω t·ª±\n",
    "MERGE_TARGET_LEN    = 250    # g·ªôp t·ªõi khi >= N k√Ω t·ª±\n",
    "\n",
    "# ===== H√ÄM H·ªñ TR·ª¢ =====\n",
    "def normalize_text(t: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", t.strip().lower())\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ===== N·∫†P D·ªÆ LI·ªÜU =====\n",
    "chunks = load_json(IN_PATH)\n",
    "n_orig = len(chunks)\n",
    "\n",
    "# ƒê·∫£m b·∫£o tr∆∞·ªùng text v√† char_len\n",
    "for c in chunks:\n",
    "    if \"text\" not in c:\n",
    "        c[\"text\"] = \"\"\n",
    "    if \"char_len\" not in c or not isinstance(c[\"char_len\"], int):\n",
    "        c[\"char_len\"] = len(c[\"text\"])\n",
    "\n",
    "lengths = [c[\"char_len\"] for c in chunks] if chunks else [0]\n",
    "\n",
    "# ===== TH·ªêNG K√ä TR∆Ø·ªöC =====\n",
    "exact_counts = Counter([c[\"text\"].strip() for c in chunks])\n",
    "exact_dup_count = sum(1 for t,cnt in exact_counts.items() if cnt>1)\n",
    "\n",
    "norm_counts = Counter([normalize_text(c[\"text\"]) for c in chunks])\n",
    "norm_dup_count = sum(1 for t,cnt in norm_counts.items() if cnt>1)\n",
    "\n",
    "underscore_pct = sum(1 for c in chunks if \"_\" in c[\"text\"]) / n_orig * 100 if n_orig else 0\n",
    "\n",
    "# ===== 1) L·ªåC CHUNK QU√Å NG·∫ÆN =====\n",
    "filtered = [c for c in chunks if c[\"char_len\"] >= SHORT_DROP_CHARLEN]\n",
    "\n",
    "# ===== 2) DEDUPE (normalized) =====\n",
    "seen = {}\n",
    "uniq = []\n",
    "for idx, c in enumerate(filtered):\n",
    "    key = normalize_text(c[\"text\"])\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen[key] = idx\n",
    "    new = dict(c)\n",
    "    new[\"_orig_index\"] = idx\n",
    "    uniq.append(new)\n",
    "\n",
    "# ===== 3) G·ªòP CHUNK NG·∫ÆN LI·ªÄN K·ªÄ c√πng source_chunk_id =====\n",
    "merged = []\n",
    "i = 0\n",
    "while i < len(uniq):\n",
    "    cur = uniq[i]\n",
    "    cur_text = cur[\"text\"]\n",
    "    cur_len = cur.get(\"char_len\", len(cur_text))\n",
    "    src = cur.get(\"source_chunk_id\")\n",
    "    if cur_len < MERGE_TARGET_LEN:\n",
    "        j = i + 1\n",
    "        merged_text = cur_text\n",
    "        merged_len = cur_len\n",
    "        while j < len(uniq) and merged_len < MERGE_TARGET_LEN and uniq[j].get(\"source_chunk_id\") == src:\n",
    "            merged_text = merged_text.rstrip() + \" \" + uniq[j][\"text\"].lstrip()\n",
    "            merged_len = len(merged_text)\n",
    "            j += 1\n",
    "        new = dict(cur)\n",
    "        new[\"text\"] = merged_text\n",
    "        new[\"char_len\"] = merged_len\n",
    "        merged.append(new)\n",
    "        i = j\n",
    "    else:\n",
    "        merged.append(cur)\n",
    "        i += 1\n",
    "\n",
    "# ===== 4) DEDUPE CU·ªêI & T·∫†O text_for_embed =====\n",
    "final = []\n",
    "seen_final = set()\n",
    "for c in merged:\n",
    "    key = normalize_text(c[\"text\"])\n",
    "    if key in seen_final:\n",
    "        continue\n",
    "    seen_final.add(key)\n",
    "    c[\"text_for_embed\"] = c[\"text\"].replace(\"_\", \" \")\n",
    "    final.append(c)\n",
    "\n",
    "# ===== L∆ØU K·∫æT QU·∫¢ =====\n",
    "save_json(final, OUT_PATH)\n",
    "\n",
    "# ===== B√ÅO C√ÅO IN RA =====\n",
    "lengths_all = [c[\"char_len\"] for c in final] if final else [0]\n",
    "median_len = statistics.median(lengths_all)\n",
    "mean_len = statistics.mean(lengths_all)\n",
    "min_len = min(lengths_all)\n",
    "max_len = max(lengths_all)\n",
    "\n",
    "very_long_count = sum(1 for L in lengths_all if L > 2000)\n",
    "\n",
    "buckets = {\n",
    "    \"0-49\": sum(1 for L in lengths_all if 0 <= L <= 49),\n",
    "    \"50-99\": sum(1 for L in lengths_all if 50 <= L <= 99),\n",
    "    \"100-199\": sum(1 for L in lengths_all if 100 <= L <= 199),\n",
    "    \"200-399\": sum(1 for L in lengths_all if 200 <= L <= 399),\n",
    "    \"400-799\": sum(1 for L in lengths_all if 400 <= L <= 799),\n",
    "    \"800-1999\": sum(1 for L in lengths_all if 800 <= L <= 1999),\n",
    "    \"2000+\": very_long_count\n",
    "}\n",
    "\n",
    "print(\"=== B√ÅO C√ÅO T√ìM T·∫ÆT ===\")\n",
    "print(\"Chunks ban ƒë·∫ßu:\", n_orig)\n",
    "print(\"Sau lo·∫°i <{} k√Ω t·ª±:\".format(SHORT_DROP_CHARLEN), len(filtered))\n",
    "print(\"Sau dedupe (pre-merge):\", len(uniq))\n",
    "print(\"Sau g·ªôp:\", len(merged))\n",
    "print(\"Chunks cu·ªëi c√πng (ƒë√£ dedupe):\", len(final))\n",
    "print(\"Exact duplicate (g·ªëc):\", exact_dup_count)\n",
    "print(\"Normalized duplicate (g·ªëc):\", norm_dup_count)\n",
    "print(\"Ph·∫ßn trƒÉm chunk c√≥ d·∫•u g·∫°ch d∆∞·ªõi (_): {:.2f}%\".format(underscore_pct))\n",
    "print(\"ƒê·ªô d√†i (min/median/mean/max):\", f\"{min_len}/{median_len:.1f}/{mean_len:.1f}/{max_len}\")\n",
    "print(\"Bucket ƒë·ªô d√†i:\", buckets)\n",
    "print(\"S·ªë chunk qu√° d√†i (>2000):\", very_long_count)\n",
    "print(\"File ƒë·∫ßu ra:\", OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c3307",
   "metadata": {},
   "source": [
    "### Ki·ªÉm tra chunks d√†i v√† replace \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a218b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks ban ƒë·∫ßu: 5165\n",
      "Chunk c√≥ '_' : 5133\n",
      "Chunk > 2000: 87\n",
      "\n",
      "=== V√≠ d·ª• 10 chunk ch·ª©a '_' (tr√≠ch ƒëo·∫°n ƒë·∫ßu 200 k√Ω t·ª±) ===\n",
      "- 'vi·ªát_nam vƒÉn_h√≥a v√† du_l·ªãch tr·∫ßn m·∫°nh_th∆∞·ªùng bi√™n_so·∫°n nguy·ªÖn_minh ti·∫øn_hi·ªáu ƒë√≠nh ph√°t_h√†nh theo th·ªèa_thu·∫≠n gi·ªØa c√¥ng_ty vƒÉn_h√≥a h∆∞∆°ng_trang v√† t√°c_gi·∫£. nghi√™m_c·∫•m m·ªçi s·ª± sao_ch√©p, tr√≠ch_d·ªãch ho·∫∑c in '\n",
      "- 'no part of_this book may be reproduced byany means without prior written permission from the publisher.'\n",
      "- 'all rights reserved. no part of_this book may be reproduced byany means without prior written permission from the publisher.'\n",
      "- 'no part of_this book may be reproduced by any means without prior written permission from the publisher.'\n",
      "- 'tr·∫ßn m·∫°nh_th∆∞·ªùng bi√™n_so·∫°n nguy·ªÖn_minh ti·∫øn_hi·ªáu ƒë√≠nh vi·ªát_nam vƒÉn_h√≥a v√† du_l·ªãch nh√†_xu·∫•t_b·∫£n th√¥ng_t·∫•n h√†_n·ªôi vi·ªát_nam vƒÉn_h√≥a v√†du l·ªãch nh√†_xu·∫•t_b·∫£n th√¥ng_t·∫•n'\n",
      "- 'l·ªùi nh√†_xu·∫•t_b·∫£n vi·ªát_nam l√† c·ª≠a_ng√µ th√¥ng ra v√πng ƒë√¥ng nam_√°. vi·ªát_nam l√† m·ªôt qu·ªëc_gia c√≥ h√†ng ng√†n nƒÉm vƒÉn_hi·∫øn v·ªõi bao_danh lam_th·∫Øng_c·∫£nh, di_t√≠ch l·ªãch_s·ª≠, vƒÉn_h√≥a r·ªông d√†i kh·∫Øp ƒë·∫•t_n∆∞·ªõc. cha_√¥ng '\n",
      "- 'v·ªõi non m·ªôt ngh√¨n trang s√°ch, t·ª´ nhi·ªÅu ngu·ªìn t∆∞_li·ªáu phong_ph√∫, so·∫°n gi·∫£ gi·ªõi_thi·ªáu v·ªõi b·∫°n_ƒë·ªçc m·ªôt b·ª©c tranh to√†n_c·∫£nh v·ªÅ n·ªÅn vƒÉn_h√≥a ƒëa_d·∫°ng v√† l√¢u_ƒë·ªùi c·ªßa d√¢n_t·ªôc vi·ªát_nam t·ª´ ƒë·ªãa_l√Ω, danh_lam_th·∫Øng'\n",
      "- 'hy_v·ªçng cu·ªën s√°ch s·∫Ω l√† m·ªôt c·∫©m_nang qu√Ω cho b·∫°n_ƒë·ªçc trong v√† ngo√†i n∆∞·ªõc mu·ªën t√¨m_hi·ªÉu v·ªÅ vƒÉn_h√≥a v√† du_l·ªãch vi·ªát_nam.'\n",
      "- 'c·ªßa 64 t·ªânh_th√†nh tr√™n c·∫£ n∆∞·ªõc. hy_v·ªçng cu·ªën s√°ch s·∫Ω l√†m·ªôt c·∫©m_nang qu√Ω cho b·∫°n_ƒë·ªçc trong v√† ngo√†i n∆∞·ªõc mu·ªën t√¨m_hi·ªÉu v·ªÅvƒÉn h√≥a v√†du l·ªãch vi·ªát_nam. xin tr√¢n_tr·ªçng gi·ªõi_thi·ªáu c√πng b·∫°n_ƒë·ªçc. nh√†_xu·∫•t_b·∫£n'\n",
      "- 'xin tr√¢n_tr·ªçng gi·ªõi_thi·ªáu c√πng b·∫°n_ƒë·ªçc. nh√†_xu·∫•t_b·∫£n th√¥ng_t·∫•n'\n",
      "\n",
      "=== V√≠ d·ª• 10 chunk qu√° d√†i (tr∆∞·ªõc t√°ch, tr√≠ch 200 k√Ω t·ª± ƒë·∫ßu) ===\n",
      "- 'b·∫£ng_tra nhanh b·∫£ng_tra nhanh c√°c t·ªânh v√† th√†nh_ph·ªë an_giang........................................................................... 282 b√†r·ªãa-v≈©ng t√†u..............................................'\n",
      "- 'c√°c t·ªânh v√† th√†nh_ph·ªë nam_ƒë·ªãnh.......................................................................... 690 ngh·ªá_an............................................................................ 706 nin'\n",
      "- 'b·∫£ng_tra nhanh b·∫£ng_tra c√°c th·∫Øng_c·∫£nh, di_t√≠ch ·∫£ichi lƒÉng........................................................................... 649 ans∆°n mi·∫øu....................................................'\n",
      "- 'th·∫Ø ngc·∫£ nh, di_t√≠ch v√†l ·ªÖh·ªôi b·∫£o_t√†ng ch√†m...................................................................... 413 b·∫£o_t√†ng ch·ª©ng_t√≠ch chi ·∫øn tranh............................................ 585 b'\n",
      "- 'bi·ªát_th·ª± b·∫£o_ƒë·∫°i.................................................................... 420 b√¨nh_nguy√™n khai_trung........................................................ 957 b√¨nh_t·∫£.....................'\n",
      "- 'b·∫£ng_tra nhanh ch·ª£ n·ªïi c√°i b√®...................................................................... 912 ch·ª£ n·ªïi ph·ª•ng_hi·ªáp.............................................................. 557 ch·ª£ tam_ƒë·ªôn'\n",
      "- 'th·∫Ø ngc·∫£ nh, di_t√≠ch v√†l_·ªÖh·ªôi ch√πa ƒë√¥ng_ng·ªç..................................................................... 541 ch√πa ƒë·ª©c_la........................................................................'\n",
      "- 'b·∫£ng_tra nhanh ch√πa linh_s∆°n( ch√πa n√∫i)( long_an)....................................... 688 ch√πa linh_th·ª©u..................................................................... 916 ch√πa linh_·ª©ng......'\n",
      "- 'th·∫Ø ngc·∫£ nh, di_t√≠ch v√†l ·ªÖh·ªôi_ch√πa qu√°n_th·∫ø √¢mb·ªì t√°t....................................................... 292 ch√πa quang_minh................................................................. 310 ch√π'\n",
      "- 'b·∫£ng_tra nhanh ch√πa vƒ©nh_nghi√™m................................................................ 577 ch√πa vƒ©nh_tr√†ng................................................................... 915 ch√πa x√†t√≥n...'\n",
      "\n",
      "S·ªë chunk ban ƒë·∫ßu >2000: 87\n",
      "S·ªë chunk v·∫´n >2000 sau t√°ch: 0 (n√™n =0)\n",
      "\n",
      "=== V√≠ d·ª• 10 chunk ƒë√£ t√°ch (tr√≠ch 200 k√Ω t·ª± ƒë·∫ßu) ===\n",
      "- 'vi·ªát_nam vƒÉn_h√≥a v√† du_l·ªãch tr·∫ßn m·∫°nh_th∆∞·ªùng bi√™n_so·∫°n nguy·ªÖn_minh ti·∫øn_hi·ªáu ƒë√≠nh ph√°t_h√†nh theo th·ªèa_thu·∫≠n gi·ªØa c√¥ng_ty vƒÉn_h√≥a h∆∞∆°ng_trang v√† t√°c_gi·∫£. nghi√™m_c·∫•m m·ªçi s·ª± sao_ch√©p, tr√≠ch_d·ªãch ho·∫∑c in '\n",
      "- 'no part of_this book may be reproduced byany means without prior written permission from the publisher.'\n",
      "- 'all rights reserved. no part of_this book may be reproduced byany means without prior written permission from the publisher.'\n",
      "- 'no part of_this book may be reproduced by any means without prior written permission from the publisher.'\n",
      "- 'tr·∫ßn m·∫°nh_th∆∞·ªùng bi√™n_so·∫°n nguy·ªÖn_minh ti·∫øn_hi·ªáu ƒë√≠nh vi·ªát_nam vƒÉn_h√≥a v√† du_l·ªãch nh√†_xu·∫•t_b·∫£n th√¥ng_t·∫•n h√†_n·ªôi vi·ªát_nam vƒÉn_h√≥a v√†du l·ªãch nh√†_xu·∫•t_b·∫£n th√¥ng_t·∫•n'\n",
      "- 'v·ªõi non m·ªôt ngh√¨n trang s√°ch, t·ª´ nhi·ªÅu ngu·ªìn t∆∞_li·ªáu phong_ph√∫, so·∫°n gi·∫£ gi·ªõi_thi·ªáu v·ªõi b·∫°n_ƒë·ªçc m·ªôt b·ª©c tranh to√†n_c·∫£nh v·ªÅ n·ªÅn vƒÉn_h√≥a ƒëa_d·∫°ng v√† l√¢u_ƒë·ªùi c·ªßa d√¢n_t·ªôc vi·ªát_nam t·ª´ ƒë·ªãa_l√Ω, danh_lam_th·∫Øng'\n",
      "- 'hy_v·ªçng cu·ªën s√°ch s·∫Ω l√† m·ªôt c·∫©m_nang qu√Ω cho b·∫°n_ƒë·ªçc trong v√† ngo√†i n∆∞·ªõc mu·ªën t√¨m_hi·ªÉu v·ªÅ vƒÉn_h√≥a v√† du_l·ªãch vi·ªát_nam.'\n",
      "- 'c·ªßa 64 t·ªânh_th√†nh tr√™n c·∫£ n∆∞·ªõc. hy_v·ªçng cu·ªën s√°ch s·∫Ω l√†m·ªôt c·∫©m_nang qu√Ω cho b·∫°n_ƒë·ªçc trong v√† ngo√†i n∆∞·ªõc mu·ªën t√¨m_hi·ªÉu v·ªÅvƒÉn h√≥a v√†du l·ªãch vi·ªát_nam. xin tr√¢n_tr·ªçng gi·ªõi_thi·ªáu c√πng b·∫°n_ƒë·ªçc. nh√†_xu·∫•t_b·∫£n'\n",
      "- 'xin tr√¢n_tr·ªçng gi·ªõi_thi·ªáu c√πng b·∫°n_ƒë·ªçc. nh√†_xu·∫•t_b·∫£n th√¥ng_t·∫•n'\n",
      "- 'trong t∆∞∆°ng_lai kh√¥ng xas ·∫Ωc√≥ tuy·∫øn ƒë∆∞·ªùng_s·∫Øt, ƒë∆∞·ªùng_b·ªô xuy√™n √°n·ªëi li·ªÅn v·ªõi c√°c n∆∞·ªõc chung_quanh.'\n",
      "\n",
      "=== B√ÅO C√ÅO CU·ªêI ===\n",
      "T·ªïng chunk sau x·ª≠ l√Ω: 5409\n",
      "S·ªë b·∫£n ghi t√°ch (original chunks c√≥ > LONG_THRESHOLD): 87\n",
      "Min/Median/Mean/Max k√Ω t·ª±: 50 330 469.00573118875946 1997\n",
      "File ƒë·∫ßu ra: ../data/pdf_chunks_cleaned_for_embed_v3.json\n"
     ]
    }
   ],
   "source": [
    "# X·ª≠ l√Ω: t√°ch chunk qu√° d√†i (>2000), replace \"_\" -> \" \" cho text_for_embed, in m·∫´u ki·ªÉm tra.\n",
    "import json, os, re\n",
    "from typing import List\n",
    "\n",
    "IN_PATH  = \"../data/pdf_chunks_cleaned_for_embed.json\"   # file hi·ªán t·∫°i\n",
    "OUT_PATH = \"../data/pdf_chunks_cleaned_for_embed_v3.json\"\n",
    "MAX_PIECE_LEN = 1000      # m·ªói ph·∫ßn sau t√°ch t·ªëi ƒëa ~1000 k√Ω t·ª± (ch·ªânh ƒë∆∞·ª£c)\n",
    "LONG_THRESHOLD = 2000     # chunk > threshold s·∫Ω b·ªã t√°ch\n",
    "SAMPLE_COUNT = 10\n",
    "\n",
    "# --- h√†m t√°ch theo c√¢u; n·∫øu c√¢u v·∫´n qu√° d√†i d√πng split theo whitespace\n",
    "SENTENCE_SPLIT_RE = re.compile(r'(?<=[\\.\\?\\!\\‡•§\\u3002])\\s+|\\n+')\n",
    "\n",
    "def split_long_text(text: str, max_len: int) -> List[str]:\n",
    "    pieces = []\n",
    "    # t√°ch theo c√¢u\n",
    "    sents = [s.strip() for s in SENTENCE_SPLIT_RE.split(text) if s.strip()]\n",
    "    cur = \"\"\n",
    "    for s in sents:\n",
    "        if len(cur) + 1 + len(s) <= max_len:\n",
    "            cur = (cur + \" \" + s).strip() if cur else s\n",
    "        else:\n",
    "            if cur:\n",
    "                pieces.append(cur)\n",
    "            # n·∫øu c√¢u ƒë∆°n d√†i h∆°n max_len, split by whitespace\n",
    "            if len(s) <= max_len:\n",
    "                cur = s\n",
    "            else:\n",
    "                words = s.split()\n",
    "                buf = \"\"\n",
    "                for w in words:\n",
    "                    if len(buf) + 1 + len(w) <= max_len:\n",
    "                        buf = (buf + \" \" + w).strip() if buf else w\n",
    "                    else:\n",
    "                        if buf:\n",
    "                            pieces.append(buf)\n",
    "                        buf = w\n",
    "                if buf:\n",
    "                    cur = buf\n",
    "                else:\n",
    "                    cur = \"\"\n",
    "    if cur:\n",
    "        pieces.append(cur)\n",
    "    # n·∫øu kh√¥ng t√°ch ƒë∆∞·ª£c c√¢u (v√≠ d·ª• text kh√¥ng c√≥ d·∫•u c√¢u), fallback split by whitespace\n",
    "    if not pieces:\n",
    "        tokens = text.split()\n",
    "        buf = \"\"\n",
    "        for t in tokens:\n",
    "            if len(buf) + 1 + len(t) <= max_len:\n",
    "                buf = (buf + \" \" + t).strip() if buf else t\n",
    "            else:\n",
    "                pieces.append(buf)\n",
    "                buf = t\n",
    "        if buf:\n",
    "            pieces.append(buf)\n",
    "    return pieces\n",
    "\n",
    "# --- t·∫£i file\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "n_orig = len(chunks)\n",
    "\n",
    "# --- th·ªëng k√™ v√† l·∫•y m·∫´u tr∆∞·ªõc x·ª≠ l√Ω\n",
    "underscore_chunks = [c for c in chunks if \"_\" in c.get(\"text\",\"\")]\n",
    "long_chunks = [c for c in chunks if c.get(\"char_len\", len(c.get(\"text\",\"\"))) > LONG_THRESHOLD]\n",
    "\n",
    "print(f\"Chunks ban ƒë·∫ßu: {n_orig}\")\n",
    "print(f\"Chunk c√≥ '_' : {len(underscore_chunks)}\")\n",
    "print(f\"Chunk > {LONG_THRESHOLD}: {len(long_chunks)}\\n\")\n",
    "\n",
    "print(\"=== V√≠ d·ª• 10 chunk ch·ª©a '_' (tr√≠ch ƒëo·∫°n ƒë·∫ßu 200 k√Ω t·ª±) ===\")\n",
    "for c in underscore_chunks[:SAMPLE_COUNT]:\n",
    "    print(\"-\", repr(c[\"text\"][:200]))\n",
    "\n",
    "print(\"\\n=== V√≠ d·ª• 10 chunk qu√° d√†i (tr∆∞·ªõc t√°ch, tr√≠ch 200 k√Ω t·ª± ƒë·∫ßu) ===\")\n",
    "for c in long_chunks[:SAMPLE_COUNT]:\n",
    "    print(\"-\", repr(c[\"text\"][:200]))\n",
    "\n",
    "# --- x·ª≠ l√Ω t√°ch\n",
    "out_chunks = []\n",
    "count_split = 0\n",
    "for c in chunks:\n",
    "    text = c.get(\"text\",\"\")\n",
    "    char_len = c.get(\"char_len\", len(text))\n",
    "    if char_len > LONG_THRESHOLD:\n",
    "        parts = split_long_text(text, MAX_PIECE_LEN)\n",
    "        # n·∫øu split tr·∫£ v·ªÅ 1 ph·∫ßn (kh√¥ng t√°ch ƒë∆∞·ª£c) v·∫´n push l·∫°i\n",
    "        if len(parts) == 1:\n",
    "            new = dict(c)\n",
    "            new[\"text\"] = parts[0]\n",
    "            new[\"char_len\"] = len(parts[0])\n",
    "            new[\"text_for_embed\"] = parts[0].replace(\"_\",\" \")\n",
    "            out_chunks.append(new)\n",
    "        else:\n",
    "            # t·∫°o c√°c chunk m·ªõi, preserve metadata, g√°n source_chunk_id_partN\n",
    "            base_id = c.get(\"source_chunk_id\", \"unknown\")\n",
    "            for i, p in enumerate(parts, start=1):\n",
    "                new = dict(c)\n",
    "                new[\"text\"] = p\n",
    "                new[\"char_len\"] = len(p)\n",
    "                new[\"source_chunk_id\"] = f\"{base_id}_part{i}\"\n",
    "                new[\"text_for_embed\"] = p.replace(\"_\",\" \")\n",
    "                out_chunks.append(new)\n",
    "            count_split += 1\n",
    "    else:\n",
    "        new = dict(c)\n",
    "        new[\"text_for_embed\"] = text.replace(\"_\",\" \")\n",
    "        out_chunks.append(new)\n",
    "\n",
    "# --- sau x·ª≠ l√Ω: in 10 v√≠ d·ª• chunk ƒë√£ t√°ch\n",
    "after_long_chunks = [c for c in out_chunks if c.get(\"char_len\",0) > LONG_THRESHOLD]\n",
    "print(f\"\\nS·ªë chunk ban ƒë·∫ßu >{LONG_THRESHOLD}: {len(long_chunks)}\")\n",
    "print(f\"S·ªë chunk v·∫´n >{LONG_THRESHOLD} sau t√°ch: {len(after_long_chunks)} (n√™n =0)\")\n",
    "\n",
    "print(\"\\n=== V√≠ d·ª• 10 chunk ƒë√£ t√°ch (tr√≠ch 200 k√Ω t·ª± ƒë·∫ßu) ===\")\n",
    "examples = [c for c in out_chunks if c.get(\"char_len\",0) <= MAX_PIECE_LEN][:SAMPLE_COUNT]\n",
    "for c in examples[:SAMPLE_COUNT]:\n",
    "    print(\"-\", repr(c[\"text\"][:200]))\n",
    "\n",
    "# --- l∆∞u file\n",
    "os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- b√°o c√°o cu·ªëi\n",
    "lens = [c.get(\"char_len\",0) for c in out_chunks]\n",
    "print(\"\\n=== B√ÅO C√ÅO CU·ªêI ===\")\n",
    "print(\"T·ªïng chunk sau x·ª≠ l√Ω:\", len(out_chunks))\n",
    "print(\"S·ªë b·∫£n ghi t√°ch (original chunks c√≥ > LONG_THRESHOLD):\", count_split)\n",
    "print(\"Min/Median/Mean/Max k√Ω t·ª±:\",\n",
    "      min(lens), \n",
    "      sorted(lens)[len(lens)//2] if lens else 0,\n",
    "      sum(lens)/len(lens) if lens else 0,\n",
    "      max(lens) if lens else 0)\n",
    "print(\"File ƒë·∫ßu ra:\", OUT_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
